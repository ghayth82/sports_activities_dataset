{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Pytorch for the PCA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('pcafilewlabels.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.555856</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000716</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>-0.000194</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>0.012813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000213</td>\n",
       "      <td>-0.000554</td>\n",
       "      <td>p1</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.556310</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>-0.000724</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>-0.000490</td>\n",
       "      <td>p1</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.555609</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000703</td>\n",
       "      <td>-0.000718</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>0.012504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>-0.000239</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000412</td>\n",
       "      <td>-0.000616</td>\n",
       "      <td>p1</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.555990</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000715</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>0.012732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000490</td>\n",
       "      <td>p1</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.555658</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000701</td>\n",
       "      <td>-0.000714</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.012477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>-0.000560</td>\n",
       "      <td>p1</td>\n",
       "      <td>a01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.555856  0.000025  0.000419  0.000038 -0.000716 -0.000726  0.001239   \n",
       "1 -0.556310  0.000028  0.000418  0.000036 -0.000713 -0.000724  0.001234   \n",
       "2 -0.555609  0.000018  0.000419  0.000021 -0.000703 -0.000718  0.001231   \n",
       "3 -0.555990  0.000029  0.000417  0.000039 -0.000715 -0.000728  0.001247   \n",
       "4 -0.555658  0.000019  0.000425  0.000024 -0.000701 -0.000714  0.001223   \n",
       "\n",
       "         7         8         9  ...         22        23        24        25  \\\n",
       "0 -0.000194  0.002798  0.012813 ...   0.001206  0.000655  0.000001 -0.000071   \n",
       "1 -0.000199  0.002731  0.012509 ...   0.001149  0.000652 -0.000043 -0.000076   \n",
       "2 -0.000156  0.002711  0.012504 ...   0.000851  0.000605 -0.000239 -0.000013   \n",
       "3 -0.000215  0.002794  0.012732 ...   0.001167  0.000664 -0.000019 -0.000037   \n",
       "4 -0.000171  0.002714  0.012477 ...   0.001020  0.000639 -0.000154 -0.000130   \n",
       "\n",
       "         26        27        28        29  30   31  \n",
       "0 -0.000121 -0.000157 -0.000213 -0.000554  p1  a01  \n",
       "1 -0.000117 -0.000103 -0.000173 -0.000490  p1  a01  \n",
       "2 -0.000219 -0.000058 -0.000412 -0.000616  p1  a01  \n",
       "3 -0.000138 -0.000130 -0.000105 -0.000490  p1  a01  \n",
       "4 -0.000097 -0.000048 -0.000214 -0.000560  p1  a01  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2 = df.iloc[:,30:32]\n",
    "X2 = X2.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.555856</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000716</td>\n",
       "      <td>-0.000726</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>-0.000194</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>0.012813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>-0.000213</td>\n",
       "      <td>-0.000554</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.556310</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>-0.000724</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000103</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>-0.000490</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.555609</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000703</td>\n",
       "      <td>-0.000718</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>0.012504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>-0.000239</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000412</td>\n",
       "      <td>-0.000616</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.555990</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000715</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>0.012732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000490</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.555658</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000701</td>\n",
       "      <td>-0.000714</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.012477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>-0.000560</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.555856  0.000025  0.000419  0.000038 -0.000716 -0.000726  0.001239   \n",
       "1 -0.556310  0.000028  0.000418  0.000036 -0.000713 -0.000724  0.001234   \n",
       "2 -0.555609  0.000018  0.000419  0.000021 -0.000703 -0.000718  0.001231   \n",
       "3 -0.555990  0.000029  0.000417  0.000039 -0.000715 -0.000728  0.001247   \n",
       "4 -0.555658  0.000019  0.000425  0.000024 -0.000701 -0.000714  0.001223   \n",
       "\n",
       "         7         8         9  ...        22        23        24        25  \\\n",
       "0 -0.000194  0.002798  0.012813 ...  0.001206  0.000655  0.000001 -0.000071   \n",
       "1 -0.000199  0.002731  0.012509 ...  0.001149  0.000652 -0.000043 -0.000076   \n",
       "2 -0.000156  0.002711  0.012504 ...  0.000851  0.000605 -0.000239 -0.000013   \n",
       "3 -0.000215  0.002794  0.012732 ...  0.001167  0.000664 -0.000019 -0.000037   \n",
       "4 -0.000171  0.002714  0.012477 ...  0.001020  0.000639 -0.000154 -0.000130   \n",
       "\n",
       "         26        27        28        29  30  31  \n",
       "0 -0.000121 -0.000157 -0.000213 -0.000554   0   0  \n",
       "1 -0.000117 -0.000103 -0.000173 -0.000490   0   0  \n",
       "2 -0.000219 -0.000058 -0.000412 -0.000616   0   0  \n",
       "3 -0.000138 -0.000130 -0.000105 -0.000490   0   0  \n",
       "4 -0.000097 -0.000048 -0.000214 -0.000560   0   0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2 = df.iloc[:,0:30].join(X2)\n",
    "X_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15    480\n",
       "18    480\n",
       "8     480\n",
       "16    480\n",
       "1     480\n",
       "9     480\n",
       "17    480\n",
       "2     480\n",
       "10    480\n",
       "3     480\n",
       "7     480\n",
       "11    480\n",
       "4     480\n",
       "12    480\n",
       "5     480\n",
       "13    480\n",
       "6     480\n",
       "14    480\n",
       "0     480\n",
       "Name: 31, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2[31].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9120, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2.iloc[:,0:30].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all the features in X, and the labels in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X_2.iloc[:,0:30].as_matrix()\n",
    "y = X_2[31].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting them into `X_train, X_test, y_train, y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 451)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 14, 14, ..., 14, 11, 14])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting them into pytorch tensors from numpy arrays, because torch works with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the training data into `long` data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train.long()\n",
    "y_test = y_test.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.LongTensor"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6840, 30]),\n",
       " torch.Size([2280, 30]),\n",
       " torch.Size([6840]),\n",
       " torch.Size([2280]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the training data ready for the iterations and epochs to occur, according to their `batch_size` and shuffling them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_samples = TensorDataset(X_train, y_train)\n",
    "test_samples = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(training_samples, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(test_samples, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Adam Optimizer. \n",
    "\n",
    "* Optimizer : Adam \n",
    "* Learning Rate : 1e-2\n",
    "* Number of Hidden layers : 1\n",
    "* Number of neurons in hidden layers : 100\n",
    "* Activation function(s) used : ReLU\n",
    "* Batch size : 64\n",
    "* Epochs : 500\n",
    "* Final loss of the last batch / neuron : 0.297\n",
    "* **Accuracy of the model : 0.8951**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 100,  loss: 0.568\n",
      "Epoch : 200,  loss: 0.434\n",
      "Epoch : 300,  loss: 0.368\n",
      "Epoch : 400,  loss: 0.342\n",
      "Epoch : 500,  loss: 0.297\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(np.random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "      \n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 30, 100, 19\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "# making our model operate at double precision.\n",
    "model = model.double()\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "# since it is multiclass classification problem, we are using CrossEntropyLoss, instead of MSEloss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-2 # alpha\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # Adam optimizers\n",
    "\n",
    "for epoch in range(500):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch : %d,  loss: %.3f' %\n",
    "              (epoch + 1, running_loss / 64))\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89517543859649118"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# pass it through the model\n",
    "X_test_var = Variable(X_test, volatile=True)\n",
    "scores = model(X_test_var)\n",
    "_, preds = torch.max(scores, dim=1)\n",
    "\n",
    "accuracy_score(preds.data.numpy(), y_test.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With RMSProp optimizer.\n",
    "* Optimizer : RMSProp \n",
    "* Learning Rate : 1e-2 \n",
    "* Number of Hidden layers : 1\n",
    "* Number of neurons in hidden layers : 100\n",
    "* Activation function(s) used : ReLU\n",
    "* Batch size : 64\n",
    "* Epochs : 500\n",
    "* Final loss of the last batch / neuron : 0.275\n",
    "* **Accuracy of the model : 0.8877**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 100,  loss: 0.549\n",
      "Epoch : 200,  loss: 0.400\n",
      "Epoch : 300,  loss: 0.331\n",
      "Epoch : 400,  loss: 0.292\n",
      "Epoch : 500,  loss: 0.275\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(np.random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "      \n",
    "\n",
    "N, D_in, H, D_out = 64, 30, 100, 19\n",
    "\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "model = model.double()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(500):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch : %d,  loss: %.3f' %\n",
    "              (epoch + 1, running_loss / 64))\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88771929824561402"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# pass it through the model\n",
    "X_test_var = Variable(X_test, volatile=True)\n",
    "scores = model(X_test_var)\n",
    "_, preds = torch.max(scores, dim=1)\n",
    "\n",
    "accuracy_score(preds.data.numpy(), y_test.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With SGD optimizer.\n",
    "\n",
    "* Optimizer : SGD \n",
    "* Learning Rate : 1e-2\n",
    "* Number of Hidden layers : 1\n",
    "* Number of neurons in hidden layers : 100\n",
    "* Activation function(s) used : ReLU\n",
    "* Batch size : 64\n",
    "* Epochs : 5000\n",
    "* Final loss of the last batch / neuron : 0.940\n",
    "* **Accuracy : 0.8451**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 100,  loss: 3.663\n",
      "Epoch : 200,  loss: 3.420\n",
      "Epoch : 300,  loss: 3.325\n",
      "Epoch : 400,  loss: 3.279\n",
      "Epoch : 500,  loss: 3.243\n",
      "Epoch : 600,  loss: 3.223\n",
      "Epoch : 700,  loss: 3.133\n",
      "Epoch : 800,  loss: 2.928\n",
      "Epoch : 900,  loss: 2.667\n",
      "Epoch : 1000,  loss: 2.539\n",
      "Epoch : 1100,  loss: 2.292\n",
      "Epoch : 1200,  loss: 2.202\n",
      "Epoch : 1300,  loss: 1.962\n",
      "Epoch : 1400,  loss: 2.043\n",
      "Epoch : 1500,  loss: 1.782\n",
      "Epoch : 1600,  loss: 1.726\n",
      "Epoch : 1700,  loss: 1.716\n",
      "Epoch : 1800,  loss: 1.617\n",
      "Epoch : 1900,  loss: 1.588\n",
      "Epoch : 2000,  loss: 1.468\n",
      "Epoch : 2100,  loss: 1.441\n",
      "Epoch : 2200,  loss: 1.460\n",
      "Epoch : 2300,  loss: 1.319\n",
      "Epoch : 2400,  loss: 1.332\n",
      "Epoch : 2500,  loss: 1.264\n",
      "Epoch : 2600,  loss: 1.284\n",
      "Epoch : 2700,  loss: 1.208\n",
      "Epoch : 2800,  loss: 1.208\n",
      "Epoch : 2900,  loss: 1.154\n",
      "Epoch : 3000,  loss: 1.208\n",
      "Epoch : 3100,  loss: 1.151\n",
      "Epoch : 3200,  loss: 1.102\n",
      "Epoch : 3300,  loss: 1.129\n",
      "Epoch : 3400,  loss: 1.087\n",
      "Epoch : 3500,  loss: 1.172\n",
      "Epoch : 3600,  loss: 1.059\n",
      "Epoch : 3700,  loss: 1.024\n",
      "Epoch : 3800,  loss: 0.986\n",
      "Epoch : 3900,  loss: 1.054\n",
      "Epoch : 4000,  loss: 1.063\n",
      "Epoch : 4100,  loss: 1.008\n",
      "Epoch : 4200,  loss: 1.037\n",
      "Epoch : 4300,  loss: 1.037\n",
      "Epoch : 4400,  loss: 1.014\n",
      "Epoch : 4500,  loss: 1.003\n",
      "Epoch : 4600,  loss: 0.970\n",
      "Epoch : 4700,  loss: 0.960\n",
      "Epoch : 4800,  loss: 0.951\n",
      "Epoch : 4900,  loss: 0.928\n",
      "Epoch : 5000,  loss: 0.940\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(np.random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "      \n",
    "\n",
    "N, D_in, H, D_out = 64, 30, 100, 19\n",
    "\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "model = model.double()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(5000):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch : %d,  loss: %.3f' %\n",
    "              (epoch + 1, running_loss / 64))\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84517543859649125"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# pass it through the model\n",
    "X_test_var = Variable(X_test, volatile=True)\n",
    "scores = model(X_test_var)\n",
    "_, preds = torch.max(scores, dim=1)\n",
    "\n",
    "accuracy_score(preds.data.numpy(), y_test.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Adam Optimizer , Karpathy constant\n",
    "\n",
    "* Optimizer : Adam \n",
    "* Learning Rate : 3e-4 # karpathy constant\n",
    "* Number of Hidden layers : 1\n",
    "* Number of neurons in hidden layers : 100\n",
    "* Activation function(s) used : ReLU\n",
    "* Batch size : 64\n",
    "* Epochs : 500\n",
    "* Final loss of the last batch / neuron : 0.938\n",
    "* **Accuracy of the model : 0.8078**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 100,  loss: 1.821\n",
      "Epoch : 200,  loss: 1.359\n",
      "Epoch : 300,  loss: 1.150\n",
      "Epoch : 400,  loss: 1.055\n",
      "Epoch : 500,  loss: 0.938\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(np.random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "      \n",
    "\n",
    "N, D_in, H, D_out = 64, 30, 100, 19\n",
    "\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "model = model.double()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(500):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch : %d,  loss: %.3f' %\n",
    "              (epoch + 1, running_loss / 64))\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80789473684210522"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# pass it through the model\n",
    "X_test_var = Variable(X_test, volatile=True)\n",
    "scores = model(X_test_var)\n",
    "_, preds = torch.max(scores, dim=1)\n",
    "\n",
    "accuracy_score(preds.data.numpy(), y_test.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy\n",
    "\n",
    "saved_trainer = copy.deepcopy(model)\n",
    "with open(r\"bestmodel_object.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(saved_trainer, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/greed/anaconda3/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type DynamicNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, './model803.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Task\n",
    "\n",
    "## Prediction of Patient with signals and activity as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# selecting the first 30 columns of the pca data and the activity column, and storing it as a matrix in X\n",
    "# getting the patient ID column from X_2 and storing it as a numpy array in y.\n",
    "\n",
    "X = X_2.iloc[:, 0:30].join(X_2[31]).as_matrix()\n",
    "y = X_2[30].as_matrix()\n",
    "\n",
    "# train_test_split to create a testing set and a validation set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 451)\n",
    "\n",
    "# converting all the numpy arrays into torch Double Tensors.\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "# For the net I've writtent takes, only long tensors for target data, so converting the labels into Long Tensors.\n",
    "y_train = y_train.long()\n",
    "y_test = y_test.long()\n",
    "\n",
    "# putting them together , preparation for training.\n",
    "training_samples = TensorDataset(X_train, y_train)\n",
    "\n",
    "# makes division using batch_size = 64, and shuffle is enabled.\n",
    "train_loader = DataLoader(training_samples, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 100,  loss: 2.876\n",
      "Epoch : 200,  loss: 2.663\n",
      "Epoch : 300,  loss: 2.607\n",
      "Epoch : 400,  loss: 2.453\n",
      "Epoch : 500,  loss: 2.409\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(np.random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "      \n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 31, 100, 8\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "# making our model operate at double precision.\n",
    "model = model.double()\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "# since it is multiclass classification problem, we are using CrossEntropyLoss, instead of MSEloss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-2 # alpha\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate) \n",
    "\n",
    "for epoch in range(500): # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch : %d,  loss: %.3f' %\n",
    "              (epoch + 1, running_loss / 64))\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50438596491228072"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# pass it through the model\n",
    "X_test_var = Variable(X_test, volatile=True)\n",
    "scores = model(X_test_var)\n",
    "_, preds = torch.max(scores, dim=1)\n",
    "\n",
    "accuracy_score(preds.data.numpy(), y_test.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Activity with the signal using NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Pytorch\n",
    "\n",
    "\n",
    "|**Model**                    |**Accuracy**|\n",
    "|-----------------------------|------------|\n",
    "|With Adam Optimizer  |0.8951|\n",
    "|With Adam Optimizer, Karpathy constant | 0.8078 |\n",
    "|With RMSProp optimizer  |0.8877|\n",
    "|With SGD Optimizer | 0.8451 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Patient with the signal and activity using NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Pytorch\n",
    "\n",
    "\n",
    "|**Model**                    |**Accuracy**|\n",
    "|-----------------------------|------------|\n",
    "|With RMSProp optimizer  |0.5043|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
